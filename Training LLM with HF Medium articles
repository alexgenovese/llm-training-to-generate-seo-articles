{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyN7+rOlW+4J6MUgkcZvFoky"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-VKD0g0aXyX","executionInfo":{"status":"ok","timestamp":1706988455927,"user_tz":-60,"elapsed":2015,"user":{"displayName":"Alex Genovese","userId":"00138227469390013470"}},"outputId":"b5c6e6b1-12e9-4c36-c2ab-afd86b916ec6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting datasets\n","  Using cached datasets-2.16.1-py3-none-any.whl (507 kB)\n","Collecting sklearn\n","  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","\u001b[31mERROR: Could not find a version that satisfies the requirement ffspec (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for ffspec\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# Import required libraries\n","!pip install torch torchvision transformers datasets sklearn gunicorn uvicorn pytorch-lightning\n","!pip install -qqq ffspec numpy pandas"]},{"cell_type":"code","source":["# Load the dataset\n","from datasets import load_dataset\n","\n","raw_dataset = load_dataset(\"fabiochiu/medium-articles\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"QCGdHhaJaaQX","executionInfo":{"status":"error","timestamp":1706988360546,"user_tz":-60,"elapsed":23,"user":{"displayName":"Alex Genovese","userId":"00138227469390013470"}},"outputId":"a8cda6ee-1c7c-4432-c551-97e2b12940a1"},"execution_count":2,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4e2a40273ca6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fabiochiu/medium-articles\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# Clean the dataset by removing unnecessary fields and keeping only the article content\n","cleaned_dataset = raw_dataset.map(lambda examples: {\"article\": examples[\"content\"]}, remove_columns=[\"id\", \"url\"])"],"metadata":{"id":"8ZzmPdpTaeQC","executionInfo":{"status":"aborted","timestamp":1706988360546,"user_tz":-60,"elapsed":17,"user":{"displayName":"Alex Genovese","userId":"00138227469390013470"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the clean dataset into JSONLines format and save it to Google Drive\n","import json\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Save the cleaned dataset\n","with open(\"/content/gdrive/MyDrive/medium_articles.jsonl\", \"w\") as outfile:\n","    for i, example in enumerate(cleaned_dataset):\n","        outfile.write(json.dumps(example))\n","        if i < len(cleaned_dataset)-2:\n","            outfile.write(\"\\n\")\n"],"metadata":{"id":"WMT0iDMFagtc","executionInfo":{"status":"aborted","timestamp":1706988360546,"user_tz":-60,"elapsed":17,"user":{"displayName":"Alex Genovese","userId":"00138227469390013470"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import additional modules\n","from transformers import T5TokenizerFast, T5ForConditionalGeneration\n","import torch\n","import random\n","import pytorch_lightning as pl\n","import numpy as np\n","import pandas as pd\n","from typing import List, Optional, Any, Dict, Union, Tuple\n","\n","# Define constants\n","MAX_LEN = 1024\n","TRAINING_PERCENTAGE = 80\n","VALIDATION_PERCENTAGE = 20\n","BATCH_SIZE = 16\n","NUM_WORKERS = 4\n","LEARNING_RATE = 5e-5\n","WARMUP_STEPS = 1000\n","TOTAL_STEPS = 10000\n","SAVE_MODELS_STEP = 1000\n","LOG_STEP = 100\n","\n","# Function to encode inputs\n","def encode_inputs(tokenizer: T5TokenizerFast, texts: List[str], max_length: int) -> List[Dict]:\n","    encoded_texts = []\n","    for text in texts:\n","        encoding = tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_tensors=\"pt\",\n","        )\n","        encoded_texts.append({\n","            \"input_ids\": encoding[\"input_ids\"].flatten(),\n","            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n","        })\n","    return encoded_texts\n","\n","# Class defining the dataset module\n","class SummaryDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings: List[Dict], decode: bool = False):\n","        self.encodings = encodings\n","        self.decode = decode\n","\n","    def __getitem__(self, idx: int) -> Dict[str, Union[List[int], List[float]]]:\n","        item = {key: val[idx].cpu().numpy().tolist() for key, val in self.encodings[idx].items()}\n","        if self.decode:\n","            item[\"decoding\"] = self.encodings[idx]['decoding'].tolist()\n","        return item\n","\n","    def __len__(self) -> int:\n","        return len(self.encodings)"],"metadata":{"id":"OzAD6JedbZDR","executionInfo":{"status":"aborted","timestamp":1706988360546,"user_tz":-60,"elapsed":16,"user":{"displayName":"Alex Genovese","userId":"00138227469390013470"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the tokenizer and model\n","tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(\"cuda\")\n","\n","# Encode the dataset\n","encoded_dataset = encode_inputs(tokenizer, cleaned_dataset[\"article\"], MAX_LEN)\n","random.shuffle(encoded_dataset)\n","num_samples = len(encoded_dataset)\n","train_size = int(np.ceil(num_samples * (TRAINING_PERCENTAGE / 100)))\n","valid_size = num_samples - train_size\n","train_dataset = SummaryDataset(encoded_dataset[:train_size], decode=False)\n","valid_dataset = SummaryDataset(encoded_dataset[-valid_size:], decode=False)\n","\n","# Define the dataloaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n","\n","# Define the Lightning Module\n","class SummarizationModule(pl.LightningModule):\n","    def __init__(self, model: T5ForConditionalGeneration, learning_rate: float, warmup_steps: int, total_steps: int):\n","        super().__init__()\n","        self.model = model\n","        self.learning_rate = learning_rate\n","        self.total_steps = total_steps\n","        self.warmup_steps = warmup_steps\n","\n","    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> dict:\n","        outputs = self.model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            return_dict=True,\n","        )\n","        return outputs\n","\n","    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> dict:\n","        input_ids = batch[0][\"input_ids\"]\n","        attention_mask = batch[0][\"attention_mask\"]\n","        targets = batch[1][:, :1].reshape(-1)\n","        outputs = self(input_ids, attention_mask)\n","        loss = outputs.loss\n","        tensorboard_logs = {'train_loss': loss}\n","        self.log_dict(tensorboard_logs)\n","        return {'loss': loss, 'progress_bar': tensorboard_logs}\n","\n","    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> dict:\n","        input_ids = batch[0][\"input_ids\"]\n","        attention_mask = batch[0][\"attention_mask\"]\n","        targets = batch[1][:, :1].reshape(-1)\n","        outputs = self(input_ids, attention_mask)\n","        loss = outputs.loss\n","        tensorboard_logs = {'val_loss': loss}\n","        self.log_dict(tensorboard_logs)\n","        return {'val_loss': loss, 'progress_bar': tensorboard_logs}\n","\n","    def validation_epoch_end(self, outputs: List[Any]) -> None:\n","        avg_loss = sum([x['val_loss'] for x in outputs]) / len(outputs)\n","        print(f'\\nValidation Loss: {avg_loss}\\n')\n","\n","    def configure_optimizers(self) -> dict:\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [{\n","                'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n","                'weight_decay': 0.01,\n","            },\n","            {\n","                'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n","                'weight_decay': 0.0\n","            }]\n","        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.learning_rate)\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.total_steps)\n","        return {\n","            'optimizer': optimizer,\n","            'lr_scheduler': {\n","                'scheduler': scheduler,\n","                'interval': 'step'\n","            }\n","        }"],"metadata":{"id":"Wm2CNzfebkMa","executionInfo":{"status":"aborted","timestamp":1706988360547,"user_tz":-60,"elapsed":17,"user":{"displayName":"Alex Genovese","userId":"00138227469390013470"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the Lightning Module\n","module = SummarizationModule(\n","    model=model,\n","    learning_rate=LEARNING_RATE,\n","    warmup_steps=WARMUP_STEPS,\n","    total_steps=TOTAL_STEPS\n",")"],"metadata":{"id":"RSHAc0UzbmNa","executionInfo":{"status":"aborted","timestamp":1706988360547,"user_tz":-60,"elapsed":17,"user":{"displayName":"Alex Genovese","userId":"00138227469390013470"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform training\n","trainer = pl.Trainer(gpus=1, limit_train_batches=1.0, limit_val_batches=1.0, max_epochs=1, progress_bar_refresh_rate=20, gradient_clip_val=1.0)\n","trainer.fit(module, train_dataloader=train_loader, val_dataloaders=valid_loader)"],"metadata":{"id":"wTP24NlvbnV9","executionInfo":{"status":"aborted","timestamp":1706988360547,"user_tz":-60,"elapsed":17,"user":{"displayName":"Alex Genovese","userId":"00138227469390013470"}}},"execution_count":null,"outputs":[]}]}